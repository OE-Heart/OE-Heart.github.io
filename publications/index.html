<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Yixin Ou (欧翌昕) </title> <meta name="author" content="Yixin Ou (欧翌昕)"> <meta name="description" content="publications by categories in reversed chronological order. generated by jekyll-scholar."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/avatar.jpg?58cd6c6c8177b2c4c6a11e4d7b5ac722"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://oe-heart.github.io/publications/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <meta name="google-site-verification" content="FNMlrk9gfe2jxeIYV2wPdQ--FeWJgzYmGOCdI56U81k"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yixin</span> Ou (欧翌昕) </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description">publications by categories in reversed chronological order. generated by jekyll-scholar.</p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Preprint</abbr> <figure> <picture> <img src="/assets/img/publication_preview/automind.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="automind.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ou2025automind" class="col-sm-8"> <div class="title"> <a href="http://arxiv.org/abs/" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">AutoMind: Adaptive Knowledgeable Agent for Automated Data Science</a> </div> <div class="author"> <em><b>Yixin Ou</b></em><sup>*</sup>, Yujie Luo<sup>*</sup>, Jingsheng Zheng, Lanning Wei, Shuofei Qiao, Jintian Zhang, Da Zheng, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Huajun Chen, Ningyu Zhang' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>ArXiv Preprint</em>, May 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/Abstract-beige?style=flat"></a> </div> <div class="abstract hidden"> <p>Large Language Model (LLM) agents have shown great potential in solving real-world data science problems. LLM–driven data science agents promise to automate the entire machine learning pipeline, yet their real-world effectiveness remains limited. Existing frameworks depend on rigid, pre-defined workflows and inflexible coding strategies; consequently, they excel only on relatively simple, classical problems and fail to capture the empirical expertise that human practitioners bring to complex, innovative tasks. In this work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework that overcomes these deficiencies through three key advances: (1) a curated expert knowledge base that grounds the agent in domain expert knowledge, (2) an agentic knowledgeable tree search algorithm that strategically explores possible solutions, and (3) a self-adaptive coding strategy that dynamically tailors code generation to task complexity. Evaluations on two automated data science benchmarks demonstrate that AutoMind delivers superior performance versus state-of-the-art baselines. Additional analyses confirm favorable effectiveness, efficiency, and qualitative solution quality, highlighting AutoMind as an efficeint and robust step toward fully automated data science.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#B22222"> <a href="https://aclanthology.org" rel="external nofollow noopener" target="_blank">ACL 2025</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/DyKnowCircuits.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="DyKnowCircuits.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ou2025llmsacquirenewknowledge" class="col-sm-8"> <div class="title"> <a href="http://arxiv.org/abs/2502.11196" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training</a> </div> <div class="author"> <em><b>Yixin Ou</b></em>, Yunzhi Yao, Ningyu Zhang, Hui Jin, Jiacheng Sun, Shumin Deng, Zhenguo Li, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Huajun Chen' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>The 63rd Annual Meeting of the Association for Computational Linguistics (Findings)</em>, Feb 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/Abstract-beige?style=flat"></a> <a href="http://arxiv.org/abs/2502.11196" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/badge/ArXiv-b31b1b.svg?logo=ArXiv"> </a> <a href="https://github.com/zjunlp/DynamicKnowledgeCircuits" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zjunlp/DynamicKnowledgeCircuits"> </a> <a href="https://zjunlp.github.io/project/DynamicKnowledgeCircuits/" rel="external nofollow noopener" target="_blank"> <img alt="Website" src="https://img.shields.io/badge/-Website-222222?style=flat&amp;logo=githubpages"> </a> <a href="https://x.com/zxlzr/status/1904382928551059786" rel="external nofollow noopener" target="_blank"><img alt="X (formerly Twitter) URL" src="https://img.shields.io/badge/Tweet-black?style=flat&amp;logo=x"></a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=QVTr5dQAAAAJ&amp;citation_for_view=QVTr5dQAAAAJ:ufrVoPGSRksC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Citations-3-4285F4?logo=googlescholar&amp;labelColor=f6f6f6" alt="3 Google Scholar citations"> </a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Despite exceptional capabilities in knowledge-intensive tasks, Large Language Models (LLMs) face a critical gap in understanding how they internalize new knowledge, particularly how to structurally embed acquired knowledge in their neural computations. We address this issue through the lens of knowledge circuit evolution, identifying computational subgraphs that facilitate knowledge storage and processing. Our systematic analysis of circuit evolution throughout continual pre-training reveals several key findings: (1) the acquisition of new knowledge is influenced by its relevance to pre-existing knowledge; (2) the evolution of knowledge circuits exhibits a distinct phase shift from formation to optimization; (3) the evolution of knowledge circuits follows a deep-to-shallow pattern. These insights not only advance our theoretical understanding of the mechanisms of new knowledge acquisition in LLMs, but also provide potential implications for improving continual pre-training strategies to enhance model performance.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Preprint</abbr> <figure> <picture> <img src="/assets/img/publication_preview/agent_symbolic_learning.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="agent_symbolic_learning.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhou2024symboliclearningenablesselfevolving" class="col-sm-8"> <div class="title"> <a href="http://arxiv.org/abs/2406.18532" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">Symbolic Learning Enables Self-Evolving Agents</a> </div> <div class="author"> Wangchunshu Zhou<sup>*</sup>, <em><b>Yixin Ou</b></em><sup>*</sup>, Shengwei Ding<sup>*</sup>, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Shuai Wang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, Yuchen Eleanor Jiang' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>ArXiv Preprint</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/Abstract-beige?style=flat"></a> <a href="http://arxiv.org/abs/2406.18532" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/badge/ArXiv-b31b1b.svg?logo=ArXiv"> </a> <a href="https://github.com/aiwaves-cn/agents" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/aiwaves-cn/agents"> </a> <a href="https://aiwaves-cn.github.io/agents/" rel="external nofollow noopener" target="_blank"> <img alt="Website" src="https://img.shields.io/badge/-Website-222222?style=flat&amp;logo=githubpages"> </a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=QVTr5dQAAAAJ&amp;citation_for_view=QVTr5dQAAAAJ:Y0pCki6q_DkC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Citations-37-4285F4?logo=googlescholar&amp;labelColor=f6f6f6" alt="37 Google Scholar citations"> </a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>The AI community has been exploring a pathway to artificial general intelligence (AGI) by developing language agents, which are complex large language models (LLMs) pipelines involving both prompting techniques and tool usage methods. While language agents have demonstrated impressive capabilities for many real-world tasks, a fundamental limitation of current language agents research is that they are model-centric, or engineering-centric. That’s to say, the progress on prompts, tools, and pipelines of language agents requires substantial manual engineering efforts from human experts rather than automatically learning from data. We believe the transition from model-centric, or engineering-centric, to data-centric, i.e., the ability of language agents to autonomously learn and evolve in environments, is the key for them to possibly achieve AGI. In this work, we introduce agent symbolic learning, a systematic framework that enables language agents to optimize themselves on their own in a data-centric way using symbolic optimizers. Specifically, we consider agents as symbolic networks where learnable weights are defined by prompts, tools, and the way they are stacked together. Agent symbolic learning is designed to optimize the symbolic network within language agents by mimicking two fundamental algorithms in connectionist learning: back-propagation and gradient descent. Instead of dealing with numeric weights, agent symbolic learning works with natural language simulacrums of weights, loss, and gradients. We conduct proof-of-concept experiments on both standard benchmarks and complex real-world tasks and show that agent symbolic learning enables language agents to update themselves after being created and deployed in the wild, resulting in self-evolving agents.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#006400"> <a href="https://naacl.org" rel="external nofollow noopener" target="_blank">NAACL 2025</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/knowagent.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="knowagent.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhu2024knowagent" class="col-sm-8"> <div class="title"> <a href="http://arxiv.org/abs/2403.03101" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">KnowAgent: Knowledge-Augmented Planning for LLM-based Agents</a> </div> <div class="author"> Yuqi Zhu, Shuofei Qiao, <em><b>Yixin Ou</b></em>, Shumin Deng, Shiwei Lyu, Yue Shen, Lei Liang, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Jinjie Gu, Huajun Chen, Ningyu Zhang' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>2025 Annual Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics (Findings)</em>, Mar 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/Abstract-beige?style=flat"></a> <a href="http://arxiv.org/abs/2403.03101" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/badge/ArXiv-b31b1b.svg?logo=ArXiv"> </a> <a href="https://github.com/zjunlp/KnowAgent" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zjunlp/KnowAgent"> </a> <a href="https://zjunlp.github.io/project/KnowAgent/" rel="external nofollow noopener" target="_blank"> <img alt="Website" src="https://img.shields.io/badge/-Website-222222?style=flat&amp;logo=githubpages"> </a> <a href="https://x.com/zxlzr/status/1765300699309752336" rel="external nofollow noopener" target="_blank"><img alt="X (formerly Twitter) URL" src="https://img.shields.io/badge/Tweet-black?style=flat&amp;logo=x"></a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=QVTr5dQAAAAJ&amp;citation_for_view=QVTr5dQAAAAJ:Tyk-4Ss8FVUC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Citations-43-4285F4?logo=googlescholar&amp;labelColor=f6f6f6" alt="43 Google Scholar citations"> </a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone models demonstrate that KnowAgent can achieve comparable or superior performance to existing baselines. Further analysis indicates the effectiveness of KnowAgent in terms of planning hallucinations mitigation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#B22222"> <a href="https://aclanthology.org" rel="external nofollow noopener" target="_blank">ACL 2024</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/easyinstruct.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="easyinstruct.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ou2024easyinstruct" class="col-sm-8"> <div class="title"> <a href="http://arxiv.org/abs/2402.03049" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models</a> </div> <div class="author"> <em><b>Yixin Ou</b></em>, Ningyu Zhang, Honghao Gui, Ziwen Xu, Shuofei Qiao, Yida Xue, Runnan Fang, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Kangwei Liu, Lei Li, Zhen Bi, others' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>The 62nd Annual Meeting of the Association for Computational Linguistics (System Demonstration Track)</em>, Feb 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/Abstract-beige?style=flat"></a> <a href="http://arxiv.org/abs/2402.03049" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/badge/ArXiv-b31b1b.svg?logo=ArXiv"> </a> <a href="https://github.com/zjunlp/EasyInstruct" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zjunlp/EasyInstruct"> </a> <a href="https://zjunlp.github.io/project/EasyInstruct/" rel="external nofollow noopener" target="_blank"> <img alt="Website" src="https://img.shields.io/badge/-Website-222222?style=flat&amp;logo=githubpages"> </a> <a href="https://www.youtube.com/watch?v=rfQOWYfziFo" rel="external nofollow noopener" target="_blank"> <img alt="Video" src="https://img.shields.io/badge/-Video-FF0000.svg?style=flat&amp;logo=youtube"> </a> <a href="https://huggingface.co/spaces/zjunlp/EasyInstruct" rel="external nofollow noopener" target="_blank"> <img alt="Huggingface Collections" src="https://img.shields.io/badge/-%F0%9F%A4%97%20Huggingface-FFD21E?style=flat"> </a> <a href="https://x.com/zxlzr/status/1754701069333205428" rel="external nofollow noopener" target="_blank"><img alt="X (formerly Twitter) URL" src="https://img.shields.io/badge/Tweet-black?style=flat&amp;logo=x"></a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=QVTr5dQAAAAJ&amp;citation_for_view=QVTr5dQAAAAJ:zYLM7Y9cAGgC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Citations-3-4285F4?logo=googlescholar&amp;labelColor=f6f6f6" alt="3 Google Scholar citations"> </a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at github, along with an online demo app and a demo video for quick-start, calling for broader research centered on instruction data and synthetic data.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#B22222"> <a href="https://aclanthology.org" rel="external nofollow noopener" target="_blank">ACL 2024</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/oceangpt.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="oceangpt.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bi2023oceangpt" class="col-sm-8"> <div class="title"> <a href="http://arxiv.org/abs/2310.02031" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">OceanGPT: A Large language Model for Ocean Science Tasks</a> </div> <div class="author"> Zhen Bi, Ningyu Zhang, Yida Xue, <em><b>Yixin Ou</b></em>, Daxiong Ji, Guozhou Zheng, and Huajun Chen </div> <div class="periodical"> <em>The 62nd Annual Meeting of the Association for Computational Linguistics</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/Abstract-beige?style=flat"></a> <a href="http://arxiv.org/abs/2310.02031" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/badge/ArXiv-b31b1b.svg?logo=ArXiv"> </a> <a href="https://github.com/zjunlp/OceanGPT" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zjunlp/OceanGPT"> </a> <a href="http://oceangpt.zjukg.cn/" rel="external nofollow noopener" target="_blank"> <img alt="Website" src="https://img.shields.io/badge/-Website-222222?style=flat&amp;logo=githubpages"> </a> <a href="https://huggingface.co/collections/zjunlp/oceangpt-664cc106358fdd9f09aa5157" rel="external nofollow noopener" target="_blank"> <img alt="Huggingface Collections" src="https://img.shields.io/badge/-%F0%9F%A4%97%20Huggingface-FFD21E?style=flat"> </a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=QVTr5dQAAAAJ&amp;citation_for_view=QVTr5dQAAAAJ:2osOgNQ5qMEC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Citations-43-4285F4?logo=googlescholar&amp;labelColor=f6f6f6" alt="43 Google Scholar citations"> </a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet’s surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reasons are the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever large language model in the ocean domain, which is expert in various ocean science tasks. We also propose OceanGPT, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though comprehensive experiments, OceanGPT not only shows a higher level of knowledge expertise for oceans science tasks but also gains preliminary embodied intelligence capabilities in ocean technology.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">WWWJ</abbr> <figure> <picture> <img src="/assets/img/publication_preview/autokg.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="autokg.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhu2024llms" class="col-sm-8"> <div class="title"> <a href="http://arxiv.org/abs/2305.13168" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities</a> </div> <div class="author"> Yuqi Zhu, Xiaohan Wang, Jing Chen, Shuofei Qiao, <em><b>Yixin Ou</b></em>, Yunzhi Yao, Shumin Deng, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Huajun Chen, Ningyu Zhang' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>World Wide Web Journal</em>, May 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/Abstract-beige?style=flat"></a> <a href="http://arxiv.org/abs/2305.13168" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/badge/ArXiv-b31b1b.svg?logo=ArXiv"> </a> <a href="https://github.com/zjunlp/AutoKG" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zjunlp/AutoKG"> </a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=QVTr5dQAAAAJ&amp;citation_for_view=QVTr5dQAAAAJ:qjMakFHDy7sC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Citations-229-4285F4?logo=googlescholar&amp;labelColor=f6f6f6" alt="229 Google Scholar citations"> </a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>This paper presents an exhaustive quantitative and qualitative evaluation of Large Language Models (LLMs) for Knowledge Graph (KG) construction and reasoning. We engage in experiments across eight diverse datasets, focusing on four representative tasks encompassing entity and relation extraction, event extraction, link prediction, and question-answering, thereby thoroughly exploring LLMs’ performance in the domain of construction and inference. Empirically, our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors. Specifically, while GPT-4 exhibits good performance in tasks related to KG construction, it excels further in reasoning tasks, surpassing fine-tuned models in certain cases. Moreover, our investigation extends to the potential generalization ability of LLMs for information extraction, leading to the proposition of a Virtual Knowledge Extraction task and the development of the corresponding VINE dataset. Based on these empirical findings, we further propose AutoKG, a multi-agent-based approach employing LLMs and external sources for KG construction and reasoning. We anticipate that this research can provide invaluable insights for future undertakings in the field of knowledge graphs.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">WWW 2023</abbr> <figure> <picture> <img src="/assets/img/publication_preview/alipaykg.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="alipaykg.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="He_2023" class="col-sm-8"> <div class="title"> <a href="http://arxiv.org/abs/2301.00503" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">A Concept Knowledge Graph for User Next Intent Prediction at Alipay</a> </div> <div class="author"> Yacheng He, Qianghuai Jia, Lin Yuan, Ruopeng Li, <em><b>Yixin Ou</b></em>, and Ningyu Zhang </div> <div class="periodical"> <em>Companion Proceedings of the ACM Web Conference 2023 (Industry Track)</em>, Jan 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/Abstract-beige?style=flat"></a> <a href="http://arxiv.org/abs/2301.00503" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/badge/ArXiv-b31b1b.svg?logo=ArXiv"> </a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=QVTr5dQAAAAJ&amp;citation_for_view=QVTr5dQAAAAJ:u5HHmVD_uO8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Citations-5-4285F4?logo=googlescholar&amp;labelColor=f6f6f6" alt="5 Google Scholar citations"> </a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>This paper illustrates the technologies of user next intent prediction with a concept knowledge graph. The system has been deployed on the Web at Alipay, serving more than 100 million daily active users. To explicitly characterize user intent, we propose AlipayKG, which is an offline concept knowledge graph in the Life-Service domain modeling the historical behaviors of users, the rich content interacted by users and the relations between them. We further introduce a Transformer-based model which integrates expert rules from the knowledge graph to infer the online user’s next intent. Experimental results demonstrate that the proposed system can effectively enhance the performance of the downstream tasks while retaining explainability.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#B22222"> <a href="https://aclanthology.org" rel="external nofollow noopener" target="_blank">ACL 2023</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/survey.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="survey.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="qiao2022reasoning" class="col-sm-8"> <div class="title"> <a href="http://arxiv.org/abs/2212.09597" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">Reasoning with Language Model Prompting: A Survey</a> </div> <div class="author"> Shuofei Qiao<sup>*</sup>, <em><b>Yixin Ou</b></em><sup>*</sup>, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Fei Huang, Huajun Chen' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>The 61st Annual Meeting of the Association for Computational Linguistics</em>, Dec 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/Abstract-beige?style=flat"></a> <a href="http://arxiv.org/abs/2212.09597" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/badge/ArXiv-b31b1b.svg?logo=ArXiv"> </a> <a href="https://github.com/zjunlp/Prompt4ReasoningPapers" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zjunlp/Prompt4ReasoningPapers"> </a> <a href="https://x.com/zxlzr/status/1654292839826882565" rel="external nofollow noopener" target="_blank"><img alt="X (formerly Twitter) URL" src="https://img.shields.io/badge/Tweet-black?style=flat&amp;logo=x"></a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=QVTr5dQAAAAJ&amp;citation_for_view=QVTr5dQAAAAJ:u-x6o8ySG0sC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Citations-317-4285F4?logo=googlescholar&amp;labelColor=f6f6f6" alt="317 Google Scholar citations"> </a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions.</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Yixin Ou (欧翌昕). Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: May 24, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>