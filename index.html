<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Yixin Ou (欧翌昕) </title> <meta name="author" content="Yixin Ou (欧翌昕)"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/avatar.jpg?58cd6c6c8177b2c4c6a11e4d7b5ac722"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://oe-heart.github.io/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <meta name="google-site-verification" content="FNMlrk9gfe2jxeIYV2wPdQ--FeWJgzYmGOCdI56U81k"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Yixin</span> Ou (欧翌昕) </h1> <p class="desc"><a href="https://www.zju.edu.cn/english/" rel="external nofollow noopener" target="_blank">Zhejiang University</a>; <a href="https://github.com/zjunlp" rel="external nofollow noopener" target="_blank">ZJUNLP</a>.</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/prof_pic.jpg?58cd6c6c8177b2c4c6a11e4d7b5ac722" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>I am now a second-year Master student of <a href="https://github.com/zjunlp" rel="external nofollow noopener" target="_blank">ZJUNLP</a> majored in Computer Science and Technology at <a href="https://www.zju.edu.cn/english/" rel="external nofollow noopener" target="_blank">Zhejiang University</a>, advised by <a href="https://person.zju.edu.cn/huajun" rel="external nofollow noopener" target="_blank">Huajun Chen (陈华钧)</a> and <a href="https://person.zju.edu.cn/ningyu" rel="external nofollow noopener" target="_blank">Ningyu Zhang (张宁豫)</a>. Previously, I graduated from the <a href="http://www.en.cs.zju.edu.cn/" rel="external nofollow noopener" target="_blank">College of Computer Science and Technology</a>, Zhejiang University (浙江大学计算机科学与技术学院) with a bachelor’s degree.</p> <p>My current research interests focus on Large Language Models (LLMs) and their theory and applications. I’m interested in the Mechanism Interpretability of LLMs and the application of LLMs to Autonomous Agents. I have published 5+ papers <a href="https://scholar.google.com/citations?user=QVTr5dQAAAAJ" rel="external nofollow noopener" target="_blank"><img src="https://img.shields.io/badge/Citations-765-4285F4?logo=googlescholar&amp;labelColor=f6f6f6&amp;style=flat"></a> at the top international NLP conferences such as ACL and NAACL.</p> <p>I will graduate in the spring of 2026 and I’m seeking a job position as a researcher or engineer of LLM algorithm.</p> </div> <div class="social"> <div class="contact-icons"> <a href="https://dblp.org/pid/336/6083.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a> <a href="mailto:%6F%75%79%69%78%69%6E@%7A%6A%75.%65%64%75.%63%6E" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/OE-Heart" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://orcid.org/0000-0001-7656-8004" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=QVTr5dQAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.semanticscholar.org/author/2196928874" title="Semantic Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar"></i></a> <a id="WeChatBtn" title="WeChat"><i class="fa-brands fa-weixin"></i></a> <div id="WeChatMod" class="wechat-modal"> <img src="/assets/img/wechat-qr.jpg" alt="WeChat QR" id="WeChatQR"> </div> <script defer src="/assets/js/wechat.js?9f2dc256d1094f6f7ffeade26e50cb50" type="text/javascript"></script> </div> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jun 16, 2025</th> <td> Start my internship at <a href="https://www.meituan.com/" rel="external nofollow noopener" target="_blank">Meituan</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">May 16, 2025</th> <td> Our paper <a href="https://arxiv.org/abs/2502.11196" rel="external nofollow noopener" target="_blank">Dynamics of Knowledge Circuits</a> is accepted by ACL 2025 (Findings)! 🎉 </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 06, 2025</th> <td> Our paper <a href="https://arxiv.org/abs/2502.11196" rel="external nofollow noopener" target="_blank">Dynamics of Knowledge Circuits</a> is accepted by <a href="https://open-foundation-model.github.io" rel="external nofollow noopener" target="_blank">SCI-FM @ ICLR 2025</a>! 🎉 </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 02, 2024</th> <td> Start my internship at Huawei Noah’s Ark Lab under the supervision of <a href="https://huijin12.github.io/" rel="external nofollow noopener" target="_blank">Hui Jin</a> and <a href="https://openreview.net/profile?id=~Jiacheng_Sun1" rel="external nofollow noopener" target="_blank">Jiacheng Sun</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 25, 2024</th> <td> We release our new work <a href="https://arxiv.org/abs/2406.18532" rel="external nofollow noopener" target="_blank">Agent Symbolic Learning</a>. Thanks for the supervision of <a href="https://michaelzhouwang.github.io/" rel="external nofollow noopener" target="_blank">Wangchunshu Zhou</a>. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Preprint</abbr> <figure> <picture> <img src="/assets/img/publication_preview/automind.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="automind.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ou2025automind" class="col-sm-8"> <div class="title"> <a href="http://arxiv.org/abs/" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">AutoMind: Adaptive Knowledgeable Agent for Automated Data Science</a> </div> <div class="author"> <em><b>Yixin Ou</b></em><sup>*</sup>, Yujie Luo<sup>*</sup>, Jingsheng Zheng, Lanning Wei, Shuofei Qiao, Jintian Zhang, Da Zheng, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Huajun Chen, Ningyu Zhang' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>ArXiv Preprint</em>, May 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/Abstract-beige?style=flat"></a> <a href="https://github.com/InnovatingAI/AutoMind" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/InnovatingAI/AutoMind"> </a> <a href="https://innovatingai.github.io/" rel="external nofollow noopener" target="_blank"> <img alt="Website" src="https://img.shields.io/badge/-Website-222222?style=flat&amp;logo=githubpages"> </a> <a href="https://x.com/zxlzr/status/1933828029035532699" rel="external nofollow noopener" target="_blank"><img alt="X (formerly Twitter) URL" src="https://img.shields.io/badge/Tweet-black?style=flat&amp;logo=x"></a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=QVTr5dQAAAAJ&amp;citation_for_view=QVTr5dQAAAAJ:2506.10974" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Citations-0-4285F4?logo=googlescholar&amp;labelColor=f6f6f6" alt="0 Google Scholar citations"> </a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Large Language Model (LLM) agents have shown great potential in solving real-world data science problems. LLM–driven data science agents promise to automate the entire machine learning pipeline, yet their real-world effectiveness remains limited. Existing frameworks depend on rigid, pre-defined workflows and inflexible coding strategies; consequently, they excel only on relatively simple, classical problems and fail to capture the empirical expertise that human practitioners bring to complex, innovative tasks. In this work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework that overcomes these deficiencies through three key advances: (1) a curated expert knowledge base that grounds the agent in domain expert knowledge, (2) an agentic knowledgeable tree search algorithm that strategically explores possible solutions, and (3) a self-adaptive coding strategy that dynamically tailors code generation to task complexity. Evaluations on two automated data science benchmarks demonstrate that AutoMind delivers superior performance versus state-of-the-art baselines. Additional analyses confirm favorable effectiveness, efficiency, and qualitative solution quality, highlighting AutoMind as an efficeint and robust step toward fully automated data science.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#B22222"> <a href="https://aclanthology.org" rel="external nofollow noopener" target="_blank">ACL 2025</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/DyKnowCircuits.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="DyKnowCircuits.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ou2025llmsacquirenewknowledge" class="col-sm-8"> <div class="title"> <a href="http://arxiv.org/abs/2502.11196" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training</a> </div> <div class="author"> <em><b>Yixin Ou</b></em>, Yunzhi Yao, Ningyu Zhang, Hui Jin, Jiacheng Sun, Shumin Deng, Zhenguo Li, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Huajun Chen' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>The 63rd Annual Meeting of the Association for Computational Linguistics (Findings)</em>, Feb 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/Abstract-beige?style=flat"></a> <a href="http://arxiv.org/abs/2502.11196" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/badge/ArXiv-b31b1b.svg?logo=ArXiv"> </a> <a href="https://github.com/zjunlp/DynamicKnowledgeCircuits" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zjunlp/DynamicKnowledgeCircuits"> </a> <a href="https://zjunlp.github.io/project/DynamicKnowledgeCircuits/" rel="external nofollow noopener" target="_blank"> <img alt="Website" src="https://img.shields.io/badge/-Website-222222?style=flat&amp;logo=githubpages"> </a> <a href="https://x.com/zxlzr/status/1904382928551059786" rel="external nofollow noopener" target="_blank"><img alt="X (formerly Twitter) URL" src="https://img.shields.io/badge/Tweet-black?style=flat&amp;logo=x"></a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=QVTr5dQAAAAJ&amp;citation_for_view=QVTr5dQAAAAJ:ufrVoPGSRksC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Citations-4-4285F4?logo=googlescholar&amp;labelColor=f6f6f6" alt="4 Google Scholar citations"> </a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Despite exceptional capabilities in knowledge-intensive tasks, Large Language Models (LLMs) face a critical gap in understanding how they internalize new knowledge, particularly how to structurally embed acquired knowledge in their neural computations. We address this issue through the lens of knowledge circuit evolution, identifying computational subgraphs that facilitate knowledge storage and processing. Our systematic analysis of circuit evolution throughout continual pre-training reveals several key findings: (1) the acquisition of new knowledge is influenced by its relevance to pre-existing knowledge; (2) the evolution of knowledge circuits exhibits a distinct phase shift from formation to optimization; (3) the evolution of knowledge circuits follows a deep-to-shallow pattern. These insights not only advance our theoretical understanding of the mechanisms of new knowledge acquisition in LLMs, but also provide potential implications for improving continual pre-training strategies to enhance model performance.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Preprint</abbr> <figure> <picture> <img src="/assets/img/publication_preview/agent_symbolic_learning.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="agent_symbolic_learning.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhou2024symboliclearningenablesselfevolving" class="col-sm-8"> <div class="title"> <a href="http://arxiv.org/abs/2406.18532" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">Symbolic Learning Enables Self-Evolving Agents</a> </div> <div class="author"> Wangchunshu Zhou<sup>*</sup>, <em><b>Yixin Ou</b></em><sup>*</sup>, Shengwei Ding<sup>*</sup>, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Shuai Wang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, Yuchen Eleanor Jiang' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>ArXiv Preprint</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/Abstract-beige?style=flat"></a> <a href="http://arxiv.org/abs/2406.18532" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/badge/ArXiv-b31b1b.svg?logo=ArXiv"> </a> <a href="https://github.com/aiwaves-cn/agents" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/aiwaves-cn/agents"> </a> <a href="https://aiwaves-cn.github.io/agents/" rel="external nofollow noopener" target="_blank"> <img alt="Website" src="https://img.shields.io/badge/-Website-222222?style=flat&amp;logo=githubpages"> </a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=QVTr5dQAAAAJ&amp;citation_for_view=QVTr5dQAAAAJ:Y0pCki6q_DkC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Citations-45-4285F4?logo=googlescholar&amp;labelColor=f6f6f6" alt="45 Google Scholar citations"> </a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>The AI community has been exploring a pathway to artificial general intelligence (AGI) by developing language agents, which are complex large language models (LLMs) pipelines involving both prompting techniques and tool usage methods. While language agents have demonstrated impressive capabilities for many real-world tasks, a fundamental limitation of current language agents research is that they are model-centric, or engineering-centric. That’s to say, the progress on prompts, tools, and pipelines of language agents requires substantial manual engineering efforts from human experts rather than automatically learning from data. We believe the transition from model-centric, or engineering-centric, to data-centric, i.e., the ability of language agents to autonomously learn and evolve in environments, is the key for them to possibly achieve AGI. In this work, we introduce agent symbolic learning, a systematic framework that enables language agents to optimize themselves on their own in a data-centric way using symbolic optimizers. Specifically, we consider agents as symbolic networks where learnable weights are defined by prompts, tools, and the way they are stacked together. Agent symbolic learning is designed to optimize the symbolic network within language agents by mimicking two fundamental algorithms in connectionist learning: back-propagation and gradient descent. Instead of dealing with numeric weights, agent symbolic learning works with natural language simulacrums of weights, loss, and gradients. We conduct proof-of-concept experiments on both standard benchmarks and complex real-world tasks and show that agent symbolic learning enables language agents to update themselves after being created and deployed in the wild, resulting in self-evolving agents.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#006400"> <a href="https://naacl.org" rel="external nofollow noopener" target="_blank">NAACL 2025</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/knowagent.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="knowagent.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhu2024knowagent" class="col-sm-8"> <div class="title"> <a href="http://arxiv.org/abs/2403.03101" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">KnowAgent: Knowledge-Augmented Planning for LLM-based Agents</a> </div> <div class="author"> Yuqi Zhu, Shuofei Qiao, <em><b>Yixin Ou</b></em>, Shumin Deng, Shiwei Lyu, Yue Shen, Lei Liang, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Jinjie Gu, Huajun Chen, Ningyu Zhang' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>2025 Annual Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics (Findings)</em>, Mar 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/Abstract-beige?style=flat"></a> <a href="http://arxiv.org/abs/2403.03101" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/badge/ArXiv-b31b1b.svg?logo=ArXiv"> </a> <a href="https://github.com/zjunlp/KnowAgent" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zjunlp/KnowAgent"> </a> <a href="https://zjunlp.github.io/project/KnowAgent/" rel="external nofollow noopener" target="_blank"> <img alt="Website" src="https://img.shields.io/badge/-Website-222222?style=flat&amp;logo=githubpages"> </a> <a href="https://x.com/zxlzr/status/1765300699309752336" rel="external nofollow noopener" target="_blank"><img alt="X (formerly Twitter) URL" src="https://img.shields.io/badge/Tweet-black?style=flat&amp;logo=x"></a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=QVTr5dQAAAAJ&amp;citation_for_view=QVTr5dQAAAAJ:Tyk-4Ss8FVUC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Citations-47-4285F4?logo=googlescholar&amp;labelColor=f6f6f6" alt="47 Google Scholar citations"> </a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone models demonstrate that KnowAgent can achieve comparable or superior performance to existing baselines. Further analysis indicates the effectiveness of KnowAgent in terms of planning hallucinations mitigation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#B22222"> <a href="https://aclanthology.org" rel="external nofollow noopener" target="_blank">ACL 2024</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/easyinstruct.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="easyinstruct.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ou2024easyinstruct" class="col-sm-8"> <div class="title"> <a href="http://arxiv.org/abs/2402.03049" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models</a> </div> <div class="author"> <em><b>Yixin Ou</b></em>, Ningyu Zhang, Honghao Gui, Ziwen Xu, Shuofei Qiao, Yida Xue, Runnan Fang, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Kangwei Liu, Lei Li, Zhen Bi, others' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>The 62nd Annual Meeting of the Association for Computational Linguistics (System Demonstration Track)</em>, Feb 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/Abstract-beige?style=flat"></a> <a href="http://arxiv.org/abs/2402.03049" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/badge/ArXiv-b31b1b.svg?logo=ArXiv"> </a> <a href="https://github.com/zjunlp/EasyInstruct" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zjunlp/EasyInstruct"> </a> <a href="https://zjunlp.github.io/project/EasyInstruct/" rel="external nofollow noopener" target="_blank"> <img alt="Website" src="https://img.shields.io/badge/-Website-222222?style=flat&amp;logo=githubpages"> </a> <a href="https://www.youtube.com/watch?v=rfQOWYfziFo" rel="external nofollow noopener" target="_blank"> <img alt="Video" src="https://img.shields.io/badge/-Video-FF0000.svg?style=flat&amp;logo=youtube"> </a> <a href="https://huggingface.co/spaces/zjunlp/EasyInstruct" rel="external nofollow noopener" target="_blank"> <img alt="Huggingface Collections" src="https://img.shields.io/badge/-%F0%9F%A4%97%20Huggingface-FFD21E?style=flat"> </a> <a href="https://x.com/zxlzr/status/1754701069333205428" rel="external nofollow noopener" target="_blank"><img alt="X (formerly Twitter) URL" src="https://img.shields.io/badge/Tweet-black?style=flat&amp;logo=x"></a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=QVTr5dQAAAAJ&amp;citation_for_view=QVTr5dQAAAAJ:zYLM7Y9cAGgC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Citations-3-4285F4?logo=googlescholar&amp;labelColor=f6f6f6" alt="3 Google Scholar citations"> </a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at github, along with an online demo app and a demo video for quick-start, calling for broader research centered on instruction data and synthetic data.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#B22222"> <a href="https://aclanthology.org" rel="external nofollow noopener" target="_blank">ACL 2024</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/oceangpt.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="oceangpt.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="bi2023oceangpt" class="col-sm-8"> <div class="title"> <a href="http://arxiv.org/abs/2310.02031" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">OceanGPT: A Large language Model for Ocean Science Tasks</a> </div> <div class="author"> Zhen Bi, Ningyu Zhang, Yida Xue, <em><b>Yixin Ou</b></em>, Daxiong Ji, Guozhou Zheng, and Huajun Chen </div> <div class="periodical"> <em>The 62nd Annual Meeting of the Association for Computational Linguistics</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/Abstract-beige?style=flat"></a> <a href="http://arxiv.org/abs/2310.02031" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/badge/ArXiv-b31b1b.svg?logo=ArXiv"> </a> <a href="https://github.com/zjunlp/OceanGPT" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zjunlp/OceanGPT"> </a> <a href="http://oceangpt.zjukg.cn/" rel="external nofollow noopener" target="_blank"> <img alt="Website" src="https://img.shields.io/badge/-Website-222222?style=flat&amp;logo=githubpages"> </a> <a href="https://huggingface.co/collections/zjunlp/oceangpt-664cc106358fdd9f09aa5157" rel="external nofollow noopener" target="_blank"> <img alt="Huggingface Collections" src="https://img.shields.io/badge/-%F0%9F%A4%97%20Huggingface-FFD21E?style=flat"> </a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=QVTr5dQAAAAJ&amp;citation_for_view=QVTr5dQAAAAJ:2osOgNQ5qMEC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Citations-52-4285F4?logo=googlescholar&amp;labelColor=f6f6f6" alt="52 Google Scholar citations"> </a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet’s surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reasons are the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever large language model in the ocean domain, which is expert in various ocean science tasks. We also propose OceanGPT, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though comprehensive experiments, OceanGPT not only shows a higher level of knowledge expertise for oceans science tasks but also gains preliminary embodied intelligence capabilities in ocean technology.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#B22222"> <a href="https://aclanthology.org" rel="external nofollow noopener" target="_blank">ACL 2023</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/survey.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="survey.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="qiao2022reasoning" class="col-sm-8"> <div class="title"> <a href="http://arxiv.org/abs/2212.09597" style="color: inherit; text-decoration: none;" rel="external nofollow noopener" target="_blank">Reasoning with Language Model Prompting: A Survey</a> </div> <div class="author"> Shuofei Qiao<sup>*</sup>, <em><b>Yixin Ou</b></em><sup>*</sup>, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Fei Huang, Huajun Chen' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>The 61st Annual Meeting of the Association for Computational Linguistics</em>, Dec 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/Abstract-beige?style=flat"></a> <a href="http://arxiv.org/abs/2212.09597" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/badge/ArXiv-b31b1b.svg?logo=ArXiv"> </a> <a href="https://github.com/zjunlp/Prompt4ReasoningPapers" rel="external nofollow noopener" target="_blank"> <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zjunlp/Prompt4ReasoningPapers"> </a> <a href="https://x.com/zxlzr/status/1654292839826882565" rel="external nofollow noopener" target="_blank"><img alt="X (formerly Twitter) URL" src="https://img.shields.io/badge/Tweet-black?style=flat&amp;logo=x"></a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=QVTr5dQAAAAJ&amp;citation_for_view=QVTr5dQAAAAJ:u-x6o8ySG0sC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/Citations-332-4285F4?logo=googlescholar&amp;labelColor=f6f6f6" alt="332 Google Scholar citations"> </a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions.</p> </div> </div> </div> </li> </ol> </div> <script type="text/javascript" id="mapmyvisitors" src="https://mapmyvisitors.com/map.js?cl=ffffff&amp;w=300&amp;t=n&amp;d=7wSDMEROzUhHfQ0v5SeauhnUhJ-4sy1Z2bRA_8w-bT4"></script> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Yixin Ou (欧翌昕). Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: June 29, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>