---
---

@article{ou2025automind,
      title={AutoMind: Adaptive Knowledgeable Agent for Automated Data Science}, 
      author={Yixin Ou and Yujie Luo and Jingsheng Zheng and Lanning Wei and Shuofei Qiao and Jintian Zhang and Da Zheng and Huajun Chen and Ningyu Zhang},
      journal = "ArXiv Preprint",
      month = May,
      year = "2025",
      abstract = "Large Language Model (LLM) agents have shown great potential in solving real-world data science problems. LLMâ€“driven data science agents promise to automate the entire machine learning pipeline, yet their real-world effectiveness remains limited. Existing frameworks depend on rigid, pre-defined workflows and inflexible coding strategies; consequently, they excel only on relatively simple, classical problems and fail to capture the empirical expertise that human practitioners bring to complex, innovative tasks. In this work, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework that overcomes these deficiencies through three key advances: (1) a curated expert knowledge base that grounds the agent in domain expert knowledge, (2) an agentic knowledgeable tree search algorithm that strategically explores possible solutions, and (3) a self-adaptive coding strategy that dynamically tailors code generation to task complexity. Evaluations on two automated data science benchmarks demonstrate that AutoMind delivers superior performance versus state-of-the-art baselines. Additional analyses confirm favorable effectiveness, efficiency, and qualitative solution quality, highlighting AutoMind as an efficeint and robust step toward fully automated data science.",
      preview={automind.png},
      num_co_first_author=2,
      selected = true,
      abbr={Preprint}
}

@article{ou2025llmsacquirenewknowledge,
      title={How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training}, 
      author={Yixin Ou and Yunzhi Yao and Ningyu Zhang and Hui Jin and Jiacheng Sun and Shumin Deng and Zhenguo Li and Huajun Chen},
      journal = "The 63rd Annual Meeting of the Association for Computational Linguistics (Findings)",
      month = February,
      year = "2025",
      arxiv = "2502.11196",
      abstract = "Despite exceptional capabilities in knowledge-intensive tasks, Large Language Models (LLMs) face a critical gap in understanding how they internalize new knowledge, particularly how to structurally embed acquired knowledge in their neural computations. We address this issue through the lens of knowledge circuit evolution, identifying computational subgraphs that facilitate knowledge storage and processing. Our systematic analysis of circuit evolution throughout continual pre-training reveals several key findings: (1) the acquisition of new knowledge is influenced by its relevance to pre-existing knowledge; (2) the evolution of knowledge circuits exhibits a distinct phase shift from formation to optimization; (3) the evolution of knowledge circuits follows a deep-to-shallow pattern. These insights not only advance our theoretical understanding of the mechanisms of new knowledge acquisition in LLMs, but also provide potential implications for improving continual pre-training strategies to enhance model performance.",
      preview={DyKnowCircuits.png},
      website="https://zjunlp.github.io/project/DynamicKnowledgeCircuits/",
      github="zjunlp/DynamicKnowledgeCircuits",
      twitter="https://x.com/zxlzr/status/1904382928551059786",
      google_scholar_id="ufrVoPGSRksC",
      selected = true,
      abbr={ACL 2025}
}

@article{zhou2024symboliclearningenablesselfevolving,
      title={Symbolic Learning Enables Self-Evolving Agents}, 
      author={Wangchunshu Zhou and Yixin Ou and Shengwei Ding and Long Li and Jialong Wu and Tiannan Wang and Jiamin Chen and Shuai Wang and Xiaohua Xu and Ningyu Zhang and Huajun Chen and Yuchen Eleanor Jiang},
      journal = "ArXiv Preprint",
      month = June,
      year={2024},
      arxiv={2406.18532},
      abstract="The AI community has been exploring a pathway to artificial general intelligence (AGI) by developing language agents, which are complex large language models (LLMs) pipelines involving both prompting techniques and tool usage methods. While language agents have demonstrated impressive capabilities for many real-world tasks, a fundamental limitation of current language agents research is that they are model-centric, or engineering-centric. That's to say, the progress on prompts, tools, and pipelines of language agents requires substantial manual engineering efforts from human experts rather than automatically learning from data. We believe the transition from model-centric, or engineering-centric, to data-centric, i.e., the ability of language agents to autonomously learn and evolve in environments, is the key for them to possibly achieve AGI. In this work, we introduce agent symbolic learning, a systematic framework that enables language agents to optimize themselves on their own in a data-centric way using symbolic optimizers. Specifically, we consider agents as symbolic networks where learnable weights are defined by prompts, tools, and the way they are stacked together. Agent symbolic learning is designed to optimize the symbolic network within language agents by mimicking two fundamental algorithms in connectionist learning: back-propagation and gradient descent. Instead of dealing with numeric weights, agent symbolic learning works with natural language simulacrums of weights, loss, and gradients. We conduct proof-of-concept experiments on both standard benchmarks and complex real-world tasks and show that agent symbolic learning enables language agents to update themselves after being created and deployed in the wild, resulting in self-evolving agents.",
      preview={agent_symbolic_learning.png},
      website="https://aiwaves-cn.github.io/agents/",
      github="aiwaves-cn/agents",
      num_co_first_author=3,
      google_scholar_id="Y0pCki6q_DkC",
      selected=true,
      abbr={Preprint}
}

@article{zhu2024knowagent,
      title={KnowAgent: Knowledge-Augmented Planning for LLM-based Agents},
      author={Zhu, Yuqi and Qiao, Shuofei and Ou, Yixin and Deng, Shumin and Lyu, Shiwei and Shen, Yue and Liang, Lei and Gu, Jinjie and Chen, Huajun and Zhang, Ningyu},
      journal={2025 Annual Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics (Findings)},
      month=March,
      year={2024},
      arxiv={2403.03101},
      abstract="Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone models demonstrate that KnowAgent can achieve comparable or superior performance to existing baselines. Further analysis indicates the effectiveness of KnowAgent in terms of planning hallucinations mitigation.",
      preview={knowagent.png},
      website="https://zjunlp.github.io/project/KnowAgent/",
      github="zjunlp/KnowAgent",
      twitter="https://x.com/zxlzr/status/1765300699309752336",
      google_scholar_id="Tyk-4Ss8FVUC",
      selected = true,
      abbr={NAACL 2025}
}

@article{ou2024easyinstruct,
      title={EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models},
      author={Ou, Yixin and Zhang, Ningyu and Gui, Honghao and Xu, Ziwen and Qiao, Shuofei and Xue, Yida and Fang, Runnan and Liu, Kangwei and Li, Lei and Bi, Zhen and others},
      journal={The 62nd Annual Meeting of the Association for Computational Linguistics (System Demonstration Track)},
      month=February,
      year={2024},
      arxiv={2402.03049},
      abstract="In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at github, along with an online demo app and a demo video for quick-start, calling for broader research centered on instruction data and synthetic data.",
      preview={easyinstruct.png},
      website="https://zjunlp.github.io/project/EasyInstruct/",
      github="zjunlp/EasyInstruct",
      twitter="https://x.com/zxlzr/status/1754701069333205428",
      video="https://www.youtube.com/watch?v=rfQOWYfziFo",
      huggingface="https://huggingface.co/spaces/zjunlp/EasyInstruct",
      google_scholar_id="zYLM7Y9cAGgC",
      selected = true,
      abbr={ACL 2024}
}

@article{bi2023oceangpt,
      title={OceanGPT: A Large language Model for Ocean Science Tasks},
      author={Bi, Zhen and Zhang, Ningyu and Xue, Yida and Ou, Yixin and Ji, Daxiong and Zheng, Guozhou and Chen, Huajun},
      journal={The 62nd Annual Meeting of the Association for Computational Linguistics},
      month=October,
      year={2023},
      arxiv={2310.02031},
      abstract="Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reasons are the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever large language model in the ocean domain, which is expert in various ocean science tasks. We also propose OceanGPT, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though comprehensive experiments, OceanGPT not only shows a higher level of knowledge expertise for oceans science tasks but also gains preliminary embodied intelligence capabilities in ocean technology.",
      preview={oceangpt.png},
      website="http://oceangpt.zjukg.cn/",
      github="zjunlp/OceanGPT",
      huggingface="https://huggingface.co/collections/zjunlp/oceangpt-664cc106358fdd9f09aa5157",
      google_scholar_id="2osOgNQ5qMEC",
      selected = true,
      abbr={ACL 2024}
}

@article{zhu2024llms,
      title={LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities},
      author={Zhu, Yuqi and Wang, Xiaohan and Chen, Jing and Qiao, Shuofei and Ou, Yixin and Yao, Yunzhi and Deng, Shumin and Chen, Huajun and Zhang, Ningyu},
      journal={World Wide Web Journal},
      month = May,
      year={2023},
      arxiv = "2305.13168",
      abstract = "This paper presents an exhaustive quantitative and qualitative evaluation of Large Language Models (LLMs) for Knowledge Graph (KG) construction and reasoning. We engage in experiments across eight diverse datasets, focusing on four representative tasks encompassing entity and relation extraction, event extraction, link prediction, and question-answering, thereby thoroughly exploring LLMs' performance in the domain of construction and inference. Empirically, our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors. Specifically, while GPT-4 exhibits good performance in tasks related to KG construction, it excels further in reasoning tasks, surpassing fine-tuned models in certain cases. Moreover, our investigation extends to the potential generalization ability of LLMs for information extraction, leading to the proposition of a Virtual Knowledge Extraction task and the development of the corresponding VINE dataset. Based on these empirical findings, we further propose AutoKG, a multi-agent-based approach employing LLMs and external sources for KG construction and reasoning. We anticipate that this research can provide invaluable insights for future undertakings in the field of knowledge graphs.",
      preview={autokg.png},
      github = "zjunlp/AutoKG",
      google_scholar_id="qjMakFHDy7sC",
      selected = false,
      abbr={WWWJ},
}

@article{He_2023,
      title={A Concept Knowledge Graph for User Next Intent Prediction at Alipay},
      author={He, Yacheng and Jia, Qianghuai and Yuan, Lin and Li, Ruopeng and Ou, Yixin and Zhang, Ningyu},
      journal={Companion Proceedings of the ACM Web Conference 2023 (Industry Track)},
      month={January},
      year={2023},
      arxiv={2301.00503},
      abstract="This paper illustrates the technologies of user next intent prediction with a concept knowledge graph. The system has been deployed on the Web at Alipay, serving more than 100 million daily active users. To explicitly characterize user intent, we propose AlipayKG, which is an offline concept knowledge graph in the Life-Service domain modeling the historical behaviors of users, the rich content interacted by users and the relations between them. We further introduce a Transformer-based model which integrates expert rules from the knowledge graph to infer the online user's next intent. Experimental results demonstrate that the proposed system can effectively enhance the performance of the downstream tasks while retaining explainability.",
      preview={alipaykg.png},
      google_scholar_id="u5HHmVD_uO8C",
      selected = false,
      abbr={WWW 2023},
}

@article{qiao2022reasoning,
      title={Reasoning with Language Model Prompting: A Survey},
      author={Qiao, Shuofei and Ou, Yixin and Zhang, Ningyu and Chen, Xiang and Yao, Yunzhi and Deng, Shumin and Tan, Chuanqi and Huang, Fei and Chen, Huajun},
      journal={The 61st Annual Meeting of the Association for Computational Linguistics},
      month=December,
      year={2022},
      arxiv = "2212.09597",
      abstract = "Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions.",
      preview={survey.png},
      github = "zjunlp/Prompt4ReasoningPapers",
      twitter="https://x.com/zxlzr/status/1654292839826882565",
      num_co_first_author=2,
      google_scholar_id="u-x6o8ySG0sC",
      selected = true,
      abbr={ACL 2023},
}